{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6d123f3-b689-4dd7-959a-88cda9087854",
   "metadata": {},
   "source": [
    "# Software Development for Data Science – Coursework 2 Resit 22/23\n",
    "# Convolutional Neural Networks for Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8e930d-15d5-4331-9912-1d7e1dd4a947",
   "metadata": {},
   "source": [
    "# Introduction:\r\n",
    "\r\n",
    "In this report, we delve into the realm of Convolutional Neural Networks (CNN) for image classification. Our primary objective is to conduct a comparative study between two prominent CNN-based algorithms: LeNet and MobileNetV2. The essence of our analysis involves in-depth research, algorithm selection justification, concise explanations of their operational mechanisms, and a critical evaluation of their performance on the CIFAR-10 dataset. The integration of the CIFAR-10 dataset through TensorFlow ensures seamless compatibility within the Google Colab environment, given its constrained GPU training time of 12 hour\n",
    ".\r\n",
    "\r\n",
    "# Selected Dataset:\r\n",
    "The CIFAR-10 dataset emerges as the cornerstone of our investigation. This section underscores the rationale behind its selection, its unique characteristics, relevance to image classification, and its advantageous features for our algorithmic evaluation.\r\n",
    "\r\n",
    "## Dataset Characteristics\r\n",
    "\r\n",
    "The CIFAR-10 dataset boasts a curated collection of 60,000 32x32 color images, evenly distributed among 10 distinct classes. The dataset's balanced nature provides 6,000 images per class, encompassing a diverse range of objects – from animals to vehicles and everyday items. This diversity imbues real-world relevance, making the dataset an ideal testbed for assessing image classification algorithms.\r\n",
    "\r\n",
    "## Relevance to Image Classification\r\n",
    "\r\n",
    "The dataset's alignment with image classification is evident, given its labeled images designed for categorization. With its compact image size of 32x32 pixels, it stimulates scenarios where limited spatial information necessitates the extraction of intricate features from constrained data.\r\n",
    "\r\n",
    "## Advantages for Algorithmic Evaluation\r\n",
    "\r\n",
    "The CIFAR-10 dataset offers numerous merits that render it an excellent choice for our comparative analysis:\r\n",
    "\r\n",
    "### Realism: \r\n",
    "The dataset mirrors practical image classification scenarios, enhancing the applicability of our findings.\r\n",
    "\r\n",
    "### Benchmarking:\r\n",
    "CIFAR-10 holds the status of a benchmark dataset in computer vision. This recognition facilitates standardized performance comparison.\r\n",
    "\r\n",
    "### Complexity: \r\n",
    "Despite its modest dimensions, the dataset's diverse classes and intricate object patterns challenge algorithms, providing a robust platform for assessing their adaptability.\r\n",
    "\r\n",
    "### Resource Efficiency: \r\n",
    "The dataset's manageable image size enables efficient experimentation within Google Colab's GPU time constraints.\r\n",
    "\r\n",
    "The selection of CIFAR-10 aligns harmoniously with our pursuit of comprehensive and relevant insights. Its realism, benchmarking status, complexity, and resource efficiency culminate in an ideal choice for evaluating the performance of LeNet and MobileNetV2.\r\n",
    "\r\n",
    "# Algorithms and Justification\r\n",
    "This section delves into the core of our study by meticulously scrutinizing the two chosen CNN algorithms: LeNet and MobileNetV2. LeNet, a historical landmark, holds its significance, while MobileNetV2, a contemporary choice, is selected for its relevance and performance potential. We provide robust justifications for our selections and succinctly elucidate their operational mechanisms.\r\n",
    "\r\n",
    "## LeNet: The Historical Landmark\r\n",
    "\r\n",
    "### Algorithm Overview:\r\n",
    "LeNet, introduced by LeCun et al. in 1998 [LeCun98], marks a pivotal moment in deep learning history. Comprising seven layers, LeNet serves as the precursor to modern CNN architectures. It incorporates convolutional layers, sub-sampling layers, and fully connected layers. LeNet's origin in handwritten digit recognition reflects its hierarchical design, mirroring the visual cortex's information processing.\r\n",
    "\r\n",
    "### Justification: \r\n",
    "LeNet's selection stems from dual purposes. First, it pays homage to the genesis of CNNs and their transformative impact on machine learning. Second, evaluating LeNet against a contemporary algorithm provides insights into CNN architecture evolution, contrasting historical advancements with modern designs.\r\n",
    "\r\n",
    "## MobileNetV2: The Contemporary Choice\r\n",
    "\r\n",
    "### Algorithm Overview:\r\n",
    "MobileNetV2, our selected contemporary algorithm, offers alignment with our research goals. Renowned for its architecture breakthrough, MobileNetV2 excels in image classification. With numerous layers, it features residual blocks comprising convolutional layers, batch normalization, and skip connections.\r\n",
    "\r\n",
    "### Justification:\r\n",
    "MobileNetV2's selection is rooted in its synergy with our research objectives. Its distinctive attribute, residual connections, addresses vanishing gradient challenges, facilitating deeper model training. By juxtaposing MobileNetV2 with LeNet, we gain insights into its strengths – handling vanishing gradients and capturing intricate features – and its potential limitations compared to simpler architectures. This comparison enriches our understanding of contemporary CNN dynamics.\r\n",
    "\r\n",
    "MobileNetV2's innovative architecture and significance in deep learning render it an ideal contender for this comparative exploration, contributing to the ongoing discourse on CNNs. Our analysis proceeds by unveiling the training and testing processes of LeNet and MobileNetV2 on the CIFAR-10 dataset, unraveling their performance intricacies and enhancing our comprehension of their capabilities.\r\n",
    "As we conclude this report, the comparative analysis of LeNet and MobileNetV2 on the CIFAR-10 dataset elucidates the evolution of CNN architectures. By contrasting historical and contemporary designs, we unravel performance nuances, strengths, and limitations. Our exploration enhances the collective understanding of CNN dynamics, laying the groundwork for continued advancements in image classification and deep learning. Through this endeavor, we contribute to the discourse on convolutional neural networks, drawing valuable insights \n",
    "\n",
    "# The Training and Testing Process\r\n",
    "Both LeNet and MobileNetV2 models are trained and tested on the CIFAR-10 dataset. The training process involves compiling the models with appropriate optimizers and loss functions. We utilize the GPU provided by Google Colab for faster training. After training, we evaluate the performance of the models using accuracy metrics\n",
    "\n",
    "# Algorithmic Performance Evaluation and Comparison\r\n",
    "\r\n",
    "In this study, we conducted a comprehensive evaluation and comparison of the performance of two popular convolutional neural network (CNN) architectures, LeNet and MobileNetV2, on the challenging CIFAR-10 dataset. Our goal was to analyze their training and testing results over the course of 25 epochs and draw insights into their accuracy and validation performance.\r\n",
    "\r\n",
    "LeNet, a classic CNN architecture, exhibited promising results during training and testing. Over the 25 epochs, we observed a steady improvement in accuracy and validation scores. At the outset, LeNet achieved an accuracy of 40.01% and a validation accuracy of 47.62%. However, as the training progressed, these metrics demonstrated noticeable enhancements. By the final epoch, LeNet's accuracy had reached an impressive 81.08%, while the validation accuracy stabilized at 59.71%. These results highlight LeNet's capability to learn intricate patterns within the CIFAR-10 dataset and generalize well to previously unseen data.\r\n",
    "\r\n",
    "On the other hand, our analysis revealed that MobileNetV2 faced certain challenges when dealing with the CIFAR-10 dataset. Despite an initial accuracy of 42.91%, MobileNetV2 struggled to maintain competitive performance throughout the training process. The model exhibited fluctuations in both accuracy and validation scores, indicating difficulties in learning and generalization. By the end of the 25 epochs, MobileNetV2's accuracy reached 23.47%, with a corresponding validation accuracy of 23.47%, underscoring the need for further investigation and optimization to harness the potential of MobileNetV2 for this specific dataset.\r\n",
    "\r\n",
    "The observed differences in performance between LeNet and MobileNetV2 can be attributed to their architectural dissimilarities and complexities. LeNet, with its simpler design, demonstrated a more consistent and reliable learning curve. In contrast, MobileNetV2, with its emphasis on efficiency and reduced computational cost, seemed to struggle in capturing the intricate features present in the CIFAR-10 \n",
    "\n",
    "# Conclusion:\n",
    "In conclusion, this study provides valuable insights into the algorithmic performance of LeNet and MobileNetV2 on the CIFAR-10 dataset. While LeNet showcased its prowess in image classification tasks, MobileNetV2's performance indicated the need for potential modifications or adaptations to better suit the dataset. Further research could involve fine-tuning MobileNetV2's hyperparameters or exploring alternative architectures to enhance its performance. This comparative analysis contributes to the broader understanding of CNN architectures and their suitability for specific tasks and datasets. \n",
    "\r",
    "Tthis coursework has successfully compared the performance of LeNet and MobileNetV2 algorithms for image classification using the CIFAR-10 dataset. The insights gained from this comparative analysis contribute to the ongoing discourse on convolutional neural networks and their evolution over time. Both LeNet and MobileNetV2 have their strengths and weaknesses, highlighting the importance of selecting appropriate CNN architectures for specific tasks. Further comparisons and experiments can be conducted to explore different model architectures and hyperparameter configurations for improved performance\n",
    "\n",
    "# References:\n",
    "LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. (1998d). Gradient-based learning\r\n",
    "applied to document recognition. Proceedings of the IEEE, 86(11), 2278–2324\n",
    ". datasets..from the past and present to inform future endeavors.m the past and present to inform future endeavors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe7372f-dcfc-4dc6-9475-656078a9aca9",
   "metadata": {},
   "source": [
    "# Implementation of LeNet & MobileNetV2: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d150a4b-35d0-4def-bf55-d89995814e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "# Load and preprocess CIFAR-10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93e465a7-1531-4e5d-b760-760c79b7bf6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 1.6394 - accuracy: 0.4001 - val_loss: 1.4370 - val_accuracy: 0.4762\n",
      "Epoch 2/25\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 1.3339 - accuracy: 0.5204 - val_loss: 1.2819 - val_accuracy: 0.5384\n",
      "Epoch 3/25\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 1.2051 - accuracy: 0.5724 - val_loss: 1.2407 - val_accuracy: 0.5585\n",
      "Epoch 4/25\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 1.1296 - accuracy: 0.5993 - val_loss: 1.1976 - val_accuracy: 0.5810\n",
      "Epoch 5/25\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 1.0700 - accuracy: 0.6231 - val_loss: 1.1385 - val_accuracy: 0.6002\n",
      "Epoch 6/25\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 1.0159 - accuracy: 0.6426 - val_loss: 1.1170 - val_accuracy: 0.6042\n",
      "Epoch 7/25\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.9731 - accuracy: 0.6562 - val_loss: 1.0966 - val_accuracy: 0.6202\n",
      "Epoch 8/25\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.9347 - accuracy: 0.6711 - val_loss: 1.0963 - val_accuracy: 0.6166\n",
      "Epoch 9/25\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.8951 - accuracy: 0.6848 - val_loss: 1.1323 - val_accuracy: 0.6095\n",
      "Epoch 10/25\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.8624 - accuracy: 0.6952 - val_loss: 1.1114 - val_accuracy: 0.6187\n",
      "Epoch 11/25\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 0.8329 - accuracy: 0.7048 - val_loss: 1.1288 - val_accuracy: 0.6240\n",
      "Epoch 12/25\n",
      "1563/1563 [==============================] - 35s 22ms/step - loss: 0.8012 - accuracy: 0.7155 - val_loss: 1.2021 - val_accuracy: 0.6083\n",
      "Epoch 13/25\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 0.7780 - accuracy: 0.7259 - val_loss: 1.1375 - val_accuracy: 0.6263\n",
      "Epoch 14/25\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 0.7498 - accuracy: 0.7327 - val_loss: 1.1603 - val_accuracy: 0.6143\n",
      "Epoch 15/25\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.7239 - accuracy: 0.7433 - val_loss: 1.2274 - val_accuracy: 0.6108\n",
      "Epoch 16/25\n",
      "1563/1563 [==============================] - 35s 22ms/step - loss: 0.6999 - accuracy: 0.7522 - val_loss: 1.2188 - val_accuracy: 0.6140\n",
      "Epoch 17/25\n",
      "1563/1563 [==============================] - 34s 22ms/step - loss: 0.6772 - accuracy: 0.7586 - val_loss: 1.2804 - val_accuracy: 0.6083\n",
      "Epoch 18/25\n",
      "1563/1563 [==============================] - 35s 23ms/step - loss: 0.6577 - accuracy: 0.7670 - val_loss: 1.2554 - val_accuracy: 0.6217\n",
      "Epoch 19/25\n",
      "1563/1563 [==============================] - 34s 21ms/step - loss: 0.6311 - accuracy: 0.7741 - val_loss: 1.2750 - val_accuracy: 0.6156\n",
      "Epoch 20/25\n",
      "1563/1563 [==============================] - 34s 22ms/step - loss: 0.6141 - accuracy: 0.7813 - val_loss: 1.2790 - val_accuracy: 0.6159\n",
      "Epoch 21/25\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.5962 - accuracy: 0.7857 - val_loss: 1.3275 - val_accuracy: 0.6124\n",
      "Epoch 22/25\n",
      "1563/1563 [==============================] - 34s 22ms/step - loss: 0.5807 - accuracy: 0.7940 - val_loss: 1.3665 - val_accuracy: 0.6099\n",
      "Epoch 23/25\n",
      "1563/1563 [==============================] - 35s 22ms/step - loss: 0.5578 - accuracy: 0.8012 - val_loss: 1.4543 - val_accuracy: 0.6065\n",
      "Epoch 24/25\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.5382 - accuracy: 0.8065 - val_loss: 1.4436 - val_accuracy: 0.6076\n",
      "Epoch 25/25\n",
      "1563/1563 [==============================] - 35s 22ms/step - loss: 0.5295 - accuracy: 0.8108 - val_loss: 1.4963 - val_accuracy: 0.5971\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1e8b540f7f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "# Build LeNet model\n",
    "leNet_model = Sequential([\n",
    "    Conv2D(6, kernel_size=(5, 5), activation='relu', input_shape=(32, 32, 3)),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(16, kernel_size=(5, 5), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(120, activation='relu'),\n",
    "    Dense(84, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile and train LeNet model\n",
    "leNet_model.compile(optimizer='adam',\n",
    "                    loss='sparse_categorical_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "leNet_model.fit(x_train, y_train, epochs=25, validation_data=(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ab87f3e-8dcf-47be-8321-f3ee4b17f8fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "Epoch 1/25\n",
      "1563/1563 [==============================] - 301s 181ms/step - loss: 1.7015 - accuracy: 0.4291 - val_loss: 2.5432 - val_accuracy: 0.2115\n",
      "Epoch 2/25\n",
      "1563/1563 [==============================] - 276s 177ms/step - loss: 1.6187 - accuracy: 0.4501 - val_loss: 2.0647 - val_accuracy: 0.3937\n",
      "Epoch 3/25\n",
      "1563/1563 [==============================] - 280s 179ms/step - loss: 1.6869 - accuracy: 0.4228 - val_loss: 3.1114 - val_accuracy: 0.2045\n",
      "Epoch 4/25\n",
      "1563/1563 [==============================] - 286s 183ms/step - loss: 1.8030 - accuracy: 0.3712 - val_loss: 2.6011 - val_accuracy: 0.2292\n",
      "Epoch 5/25\n",
      "1563/1563 [==============================] - 276s 177ms/step - loss: 1.8114 - accuracy: 0.3724 - val_loss: 3.1134 - val_accuracy: 0.2380\n",
      "Epoch 6/25\n",
      "1563/1563 [==============================] - 286s 183ms/step - loss: 1.8472 - accuracy: 0.3552 - val_loss: 2.7855 - val_accuracy: 0.1598\n",
      "Epoch 7/25\n",
      "1563/1563 [==============================] - 281s 180ms/step - loss: 1.8991 - accuracy: 0.3171 - val_loss: 2.2296 - val_accuracy: 0.2888\n",
      "Epoch 8/25\n",
      "1563/1563 [==============================] - 281s 180ms/step - loss: 1.8013 - accuracy: 0.3475 - val_loss: 2.4841 - val_accuracy: 0.3338\n",
      "Epoch 9/25\n",
      "1563/1563 [==============================] - 271s 173ms/step - loss: 1.7886 - accuracy: 0.3540 - val_loss: 2.8278 - val_accuracy: 0.1562\n",
      "Epoch 10/25\n",
      "1563/1563 [==============================] - 266s 170ms/step - loss: 1.7520 - accuracy: 0.3832 - val_loss: 1.7361 - val_accuracy: 0.3576\n",
      "Epoch 11/25\n",
      "1563/1563 [==============================] - 265s 170ms/step - loss: 1.8238 - accuracy: 0.3577 - val_loss: 2.8389 - val_accuracy: 0.2244\n",
      "Epoch 12/25\n",
      "1563/1563 [==============================] - 308s 197ms/step - loss: 1.8712 - accuracy: 0.3340 - val_loss: 2.0252 - val_accuracy: 0.2828\n",
      "Epoch 13/25\n",
      "1563/1563 [==============================] - 313s 200ms/step - loss: 1.7390 - accuracy: 0.3740 - val_loss: 1.6142 - val_accuracy: 0.4074\n",
      "Epoch 14/25\n",
      "1563/1563 [==============================] - 257s 164ms/step - loss: 2.0828 - accuracy: 0.2636 - val_loss: 2.1477 - val_accuracy: 0.1709\n",
      "Epoch 15/25\n",
      "1563/1563 [==============================] - 260s 167ms/step - loss: 2.0372 - accuracy: 0.2641 - val_loss: 2.0838 - val_accuracy: 0.2278\n",
      "Epoch 16/25\n",
      "1563/1563 [==============================] - 284s 182ms/step - loss: 2.0319 - accuracy: 0.2681 - val_loss: 3.5265 - val_accuracy: 0.2155\n",
      "Epoch 17/25\n",
      "1563/1563 [==============================] - 295s 189ms/step - loss: 1.9096 - accuracy: 0.3106 - val_loss: 2.0580 - val_accuracy: 0.2364\n",
      "Epoch 18/25\n",
      "1563/1563 [==============================] - 302s 193ms/step - loss: 1.9359 - accuracy: 0.3051 - val_loss: 2.1005 - val_accuracy: 0.2514\n",
      "Epoch 19/25\n",
      "1563/1563 [==============================] - 2044s 1s/step - loss: 1.9164 - accuracy: 0.3003 - val_loss: 2.1545 - val_accuracy: 0.2546\n",
      "Epoch 20/25\n",
      "1563/1563 [==============================] - 289s 185ms/step - loss: 1.9648 - accuracy: 0.2902 - val_loss: 1.9709 - val_accuracy: 0.2592\n",
      "Epoch 21/25\n",
      "1563/1563 [==============================] - 275s 176ms/step - loss: 1.9025 - accuracy: 0.3152 - val_loss: 2.1350 - val_accuracy: 0.2517\n",
      "Epoch 22/25\n",
      "1563/1563 [==============================] - 271s 173ms/step - loss: 1.8427 - accuracy: 0.3324 - val_loss: 1.7226 - val_accuracy: 0.3568\n",
      "Epoch 23/25\n",
      "1563/1563 [==============================] - 271s 173ms/step - loss: 1.8582 - accuracy: 0.3330 - val_loss: 1.8985 - val_accuracy: 0.3246\n",
      "Epoch 24/25\n",
      "1563/1563 [==============================] - 285s 182ms/step - loss: 1.9331 - accuracy: 0.3163 - val_loss: 2.1107 - val_accuracy: 0.2496\n",
      "Epoch 25/25\n",
      "1563/1563 [==============================] - 277s 177ms/step - loss: 1.8672 - accuracy: 0.3290 - val_loss: 2.1399 - val_accuracy: 0.2347\n",
      "313/313 - 9s - loss: 2.1399 - accuracy: 0.2347 - 9s/epoch - 27ms/step\n",
      "313/313 [==============================] - 10s 29ms/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load and preprocess CIFAR-10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "# MobileNetV2 architecture\n",
    "base_model = MobileNetV2(input_shape=(32, 32, 3), include_top=False, weights='imagenet')\n",
    "x = base_model.output\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "output = Dense(10, activation='softmax')(x)\n",
    "mobileNetV2_model = tf.keras.Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "# Compile and train MobileNetV2 model\n",
    "mobileNetV2_model.compile(optimizer='adam',\n",
    "                          loss='sparse_categorical_crossentropy',\n",
    "                          metrics=['accuracy'])\n",
    "\n",
    "mobileNetV2_model.fit(x_train, y_train, epochs=25, validation_data=(x_test, y_test))\n",
    "mobileNetV2_test_loss, mobileNetV2_test_accuracy = mobileNetV2_model.evaluate(x_test, y_test, verbose=2)\n",
    "mobileNetV2_predictions = mobileNetV2_model.predict(x_test)\n",
    "mobileNetV2_predictions = [tf.argmax(pred).numpy() for pred in mobileNetV2_predictions]\n",
    "mobileNetV2_accuracy = accuracy_score(y_test, mobileNetV2_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d11441-8405-43e6-bac5-2bba4ac44585",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
